{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is a neural Turing Machine?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  The basic idea of NTM is to couple a neural network to external memory resources, which can then interact with by attentional process.\n",
    "\n",
    "* The combined system is analogous to a Turing Machine or Von-Neumann Architecture, but is differentiable end-to-end\n",
    "\n",
    "* This allows training it end to end, with a  gradient descent technique\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Some Intro:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The computer programs uses three fundamental units for achieving Von-Neumann cycle:\n",
    "    -  Elementary Ops [eg:Arithmetic/Logical Operations]\n",
    "    -  Control Flow\n",
    "    -  External memory\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* But the current Machine Learning techniques gives little to no emphasis on the last two parts.\n",
    "\n",
    "\n",
    "* However RNNs are different from others in the sense, they can learn from previous sequence data, and it's transformations over extended periods of time.\n",
    "\n",
    "* This concept is practically enriched in NTM concepts by introducing large addressable memory,an analogy to turing's enrichment of finite state machines by infinite memory tape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> So how is NTM different from a turing machine?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unlike turing machine , NTM is totally differentiable.\n",
    "\n",
    "* So what this means is it can be trained , well using a gradient descent. Dude that's awesome rite?\n",
    "    - A computer that can be taught to learn a program, like copy ,sort etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> A look into human cognition? </b>\n",
    "\n",
    "[1] Working Memory:\n",
    "       -  In human cognition, a 'working memory' abstractly refers to a unit that can temporarily(short-term) store             information and can do a rule based manipulation over it.\n",
    "       -  An NTM  uses an attention based technique to read and write memory selectively\n",
    "       \n",
    "[2] In neuroscience, the working memory process has been ascribed to the functioning of a system compossed of the prefrontal cortex and basal ganglia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LSTM - The ADAM and the EVE of working memory !!!</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This very architecture is built with only one goal:- Address vanishing and exploding gradient problem. [can be relabelled as vanishing and exploding sensitivity ]\n",
    "\n",
    "* The story starts from Mr.RNN, whO is a very funny and awesome guy. He looks similiar to Mr.HMM, Hidden Markov Model. \n",
    "    - They both are good at modelling the dynamic state nature of an fsm.\n",
    "    - They key difference is that RNN uses a distributed representation of states, which offers him a significant memory and computational capacity\n",
    "    - Dynamic state is crucial , in the sense, it helps keeping context dependent computation.\n",
    "    \n",
    "* Now Mr.RNN needs to be upgraded and gave us LSTM.\n",
    "\n",
    "* LSTM solves the problem of vanishing/exploding gradients by embedding perfect integrators for memory storage in the network\n",
    "    - A simple example for perfect integrator is :\n",
    "            x(t+1) = x(t) + i(t) \n",
    "                    where i(t) is the input\n",
    "    - If we attach a mechanism to this integrator , that allows an enclosing network to chose when the integrator listens to the input, a programmable gate, depending on the context, we have a new eqn:\n",
    "            x(t+1) = x(t) + g(context)i(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>And gentlemen, let's welcome the new hero, the one and only - NTM </b></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An NTM contains mainly 2 parts:\n",
    "\n",
    "    (1) A neural network controller:\n",
    "        -  Like most neural network, controller interacts with the external world via input and output vectors\n",
    "        -  Unlike standard network, it also interacts with a memory matrix using selective read and write ops.\n",
    "        -  By analogy to the turing machine , we refer to the network outputs that parametrise these ops as \n",
    "            'heads'\n",
    "       \n",
    "    (2) Memory Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture Diagram of NTM <br><IMG SRC='NTM_Architecture.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
