{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from six.moves import range\n",
    "import string\n",
    "import os\n",
    "import zipfile\n",
    "from six.moves.urllib.request import urlretrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'text8.zip'\n",
    "\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename)as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "\n",
    "text = read_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# Now create a validation set\n",
    "valid_size=1000\n",
    "valid_text= text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Utility func for char_to_id and vice versa\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1\n",
    "first_letter_int  = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter_int + 1\n",
    "    elif char==' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected Character %s' %char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(id):\n",
    "    if id>0 :\n",
    "        return chr(id + first_letter_int -1)\n",
    "    else:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected Character ,\n",
      "0\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "print(char2id(','))\n",
    "print(id2char(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Func to generate a training batch for LSTM Model\n",
    "\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self,text,batch_size,num_unrollings):\n",
    "        self._text=text\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        num_batches = self._text_size // batch_size\n",
    "        self._cursor = [offset *  num_batches for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size,vocabulary_size),dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b,char2id(self._text[self._cursor[b]])]=1.0\n",
    "            self._cursor[b] = (self._cursor[b]+1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "def characters(probabilities):\n",
    "    return [id2char(i) for i in np.argmax(probabilities,1)]\n",
    "    \n",
    "def batches2string(batches):\n",
    "    s=['']*batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s,characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text,batch_size,num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions,labels):\n",
    "    predictions[predictions< 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels,-np.log(predictions)))/labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    r = random.uniform(0,1)\n",
    "    s=0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s > r:\n",
    "            return i\n",
    "    return len(distribution)-1\n",
    "\n",
    "def sample(prediction):\n",
    "    p = np.zeros(shape=[1,vocabulary_size],dtype=np.float)\n",
    "    p[0,sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    b = np.random.uniform(0.0,1.0,size=[1,vocabulary_size])\n",
    "    return b/ np.sum(b,1)[:,None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we'll design a simple LSTM Model\n",
    "\n",
    "\n",
    "num_nodes=64\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Define hyper params\n",
    "    # Input gate: inp,prev,bias\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size,num_nodes],-0.1,0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    ib = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    # Forget gate: inp,prev,bias\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size,num_nodes],-0.1,0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    fb = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    # Memory cell: inp,state,bias\n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size,num_nodes],-0.1,0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    cb = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #Output Gate : inp,prev,bias\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size,num_nodes],-0.1,0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes,num_nodes],-0.1,0.1))\n",
    "    ob = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    #Variables saving states across unrollings\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size,num_nodes]),trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and bias\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes,vocabulary_size],-0.1,0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    \n",
    "    #Def of cell computation\n",
    "    def lstm_cell(i,o,state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(i,ix) + tf.matmul(o,im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i,fx)+tf.matmul(o,fm) + fb)\n",
    "        update = tf.matmul(i,cx)+tf.matmul(o,cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i,ox)+tf.matmul(o,om)+ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings+1):\n",
    "        train_data.append(tf.placeholder(dtype=tf.float32,shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:] # labels are inputs shifted by one time step.\n",
    "    \n",
    "    #Unrolled lstm loop\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output,state = lstm_cell(i,output,state)\n",
    "        outputs.append(state)\n",
    "    \n",
    "    # State savings across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,axis=0),w,b)\n",
    "        loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,axis=0),logits=logits))\n",
    "        \n",
    "    \n",
    "    #Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0,global_step,5000,0.1,staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients,v = zip(*optimizer.compute_gradients(loss)) # * here to unpack coz comp_grads(..) returns list(tuple(gradient,variable))\n",
    "    gradients,_ = tf.clip_by_global_norm(gradients,1.25) # deal with exploding gradient problem\n",
    "    \n",
    "    optimizer = optimizer.apply_gradients(zip(gradients,v),global_step=global_step)\n",
    "    \n",
    "    \n",
    "    # predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    #sampling and validating eval..no unrolling since not back prop\n",
    "    \n",
    "    sample_input  = tf.placeholder(shape=[1,vocabulary_size],dtype=tf.float32)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    saved_sample_state =  tf.Variable(tf.zeros([1,num_nodes]))\n",
    "    \n",
    "    reset_sample_state=tf.group(saved_sample_output.assign(tf.zeros([1,num_nodes])),saved_sample_state.assign(tf.zeros([1,num_nodes])))\n",
    "    sample_output,sample_state = lstm_cell(sample_input,saved_sample_output,saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction  = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b)) # <- This guy is the fk'n classifier ..!! ding dong\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>Refer to this diagram for above implementation details of lstm_cell [Forget,Input,Update(Memory),Output &lt;- Gates in this order]</b><br><img src='lstm.png' width=600px  height=400px/></br>\n",
    "<br>\n",
    " <p style=\"border-style:1px double red;border-radius: 10px;border-width: medium;\" > \n",
    " &nbsp;&nbsp;<span style=\"color:green\"><b> def </b></span> <span style=\"color:blue\">lstm_cell</span>(i,o,state):<br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; input_gate = tf.sigmoid(tf.matmul(i,ix) + tf.matmul(o,im) + ib) <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; forget_gate = tf.sigmoid(tf.matmul(i,fx)+tf.matmul(o,fm) + fb) <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; update = tf.matmul(i,cx)+tf.matmul(o,cm) + cb <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; state = forget_gate * state + input_gate * tf.tanh(update) <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; output_gate = tf.sigmoid(tf.matmul(i,ox)+tf.matmul(o,om)+ob) <br>\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return output_gate * tf.tanh(state), state <br>\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global vars initialized\n",
      "Average loss at step 0: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 27.02\n",
      "================================================================================\n",
      "  sb s  sbcq  prg i w h vldumiehr ewuhchncepehqsnu fvmye r kei phityylza k bhnsmav\n",
      "vsq ezogbtdanku p o pegmihk  nbaa zddev  cieuy  hpfeuy  key  esr ir ayo xl tvxsmlt\n",
      "tueml qmn cnitcx aihoez dfamqsw jbeig   yajzop fiqzzexwi l q tttyhv  sesozvbge vcr\n",
      "rih vcty bkz s  u rfvowdn kedvwuoa oezs uxblc     dntiant  vet y zvlrse atuhpsxozy\n",
      "yystxtucyqcmrfxheo  ot l ersl lxexs rhxnweeqrmeterl m osjai   c  rtwy wra dvsu iug\n",
      "================================================================================\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 100: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 11.25\n",
      "Average loss at step 200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 9.27\n",
      "Average loss at step 300: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 7.53\n",
      "Average loss at step 400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 8.54\n",
      "Average loss at step 500: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 7.26\n",
      "Average loss at step 600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.67\n",
      "Average loss at step 700: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 7.10\n",
      "Average loss at step 800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.37\n",
      "Average loss at step 900: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.78\n",
      "Average loss at step 1000: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.08\n",
      "================================================================================\n",
      "gkoe thise farboighon to see parit the eat marviss cnitime it hat lackous ravy hos\n",
      "s the from e m be mest camaacan an our netams of to a seeivatio ly aruce two semev\n",
      "vjain eimit inments ark thriisrem inven three in s livelikes frulis on ope othe an\n",
      "nusios orwalmecideiot of tomemine four preas une one fiddin hus to yunse wreceets \n",
      " arl on litcoun asbenales the he cald two  nox nustelia the aloss s prare i saistu\n",
      "================================================================================\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 1100: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.18\n",
      "Average loss at step 1200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 7.25\n",
      "Average loss at step 1300: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 7.13\n",
      "Average loss at step 1400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.46\n",
      "Average loss at step 1500: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.01\n",
      "Average loss at step 1600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.00\n",
      "Average loss at step 1700: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.83\n",
      "Average loss at step 1800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.81\n",
      "Average loss at step 1900: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.73\n",
      "Average loss at step 2000: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.94\n",
      "================================================================================\n",
      "uo ixidil sasios anvee cailin s ateciva lisig ausooos relionousiffs mikin hewings \n",
      " queds orgiets poitsoitiesa tnoipce three eaelxs accrely thet vact ismon it horiat\n",
      "tftasiols theirs the rejest oftd oldeonima recines in recourciares coosida orfamal\n",
      "lqbi in catoura re vais iffoisss an isiesmay veldia see care to strey toinosofts g\n",
      "gquiree reji yastth and wither oftchoite marisiogia espectl c to soin weres turer \n",
      "================================================================================\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 2100: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.60\n",
      "Average loss at step 2200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.63\n",
      "Average loss at step 2300: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.31\n",
      "Average loss at step 2400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.66\n",
      "Average loss at step 2500: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.19\n",
      "Average loss at step 2600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.27\n",
      "Average loss at step 2700: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.09\n",
      "Average loss at step 2800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.94\n",
      "Average loss at step 2900: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.05\n",
      "Average loss at step 3000: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.13\n",
      "================================================================================\n",
      " joiciaphoine ingolitlee metoods tersely lice flufstor seackire oesers six s tours\n",
      "sjassions inciseiate are shcalld teira vabela i limisibles eigitier tcreatecse aur\n",
      "rfrean byes scableteshous also joi achld toila bersaoleesnetedance eietrors bircer\n",
      "rapits of tweet fataphs aiocencrilar lelvnoseelated ceftiatiougeasens aass parrual\n",
      "ljocodewible its ourts or g eicels its watitee  casnera liokedt indesection tht a \n",
      "================================================================================\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 3100: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.02\n",
      "Average loss at step 3200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 6.04\n",
      "Average loss at step 3300: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.25\n",
      "Average loss at step 3400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.09\n",
      "Average loss at step 3500: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.31\n",
      "Average loss at step 3600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.74\n",
      "Average loss at step 3700: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.46\n",
      "Average loss at step 3800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.67\n",
      "Average loss at step 3900: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.01\n",
      "Average loss at step 4000: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.55\n",
      "================================================================================\n",
      " ptiolisal eightrisitesie the equacraing wkindwork bayanitsers in one intil uth pa\n",
      "aea fcard of pipeas elec ofs serrewa tougraenersie oshs histomo zamiks txrmards wr\n",
      "rcmb layot tc domem siovilut irinaar allo emp ass america beingses nowyease allerg\n",
      "gthiney casprerco fj a calewa cooses aporbbb sed ansabefs setaneed is basiturergsp\n",
      "ph inseeby the foree yow oct loarts see a ni arroa a ofiars etlayevical kit coase \n",
      "================================================================================\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 4100: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.19\n",
      "Average loss at step 4200: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.80\n",
      "Average loss at step 4300: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.85\n",
      "Average loss at step 4400: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.55\n",
      "Average loss at step 4500: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 5.37\n",
      "Average loss at step 4600: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.81\n",
      "Average loss at step 4700: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.98\n",
      "Average loss at step 4800: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.96\n",
      "Average loss at step 4900: 1.000000 learning rate: 10.000000\n",
      "Minibatch perplexity : 4.58\n",
      "Average loss at step 5000: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 6.42\n",
      "================================================================================\n",
      " ry fais fordus eace anarieass fouren anea on ofdeen boean tarded a csurenc alhin \n",
      " amsoo can lincopieees there alonowaine beloass hasuectass muss rusti in pai a one\n",
      "enans calls once atrogaties is actrasel may as essse oriearss zesseleer cussmo wai\n",
      "ixe ter lon sout are spirase pre ntn imfout teco tyeral beigiimlicualasto diffeata\n",
      "azysesea tiloreap ko a seveience oleys leise outlearlys as outs as tegibat itses t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 5100: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.90\n",
      "Average loss at step 5200: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 5.40\n",
      "Average loss at step 5300: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 5.11\n",
      "Average loss at step 5400: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.64\n",
      "Average loss at step 5500: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.79\n",
      "Average loss at step 5600: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.73\n",
      "Average loss at step 5700: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 6.01\n",
      "Average loss at step 5800: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 5.06\n",
      "Average loss at step 5900: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 5.46\n",
      "Average loss at step 6000: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 5.28\n",
      "================================================================================\n",
      "ty lyy amecon in unrilabsedry in doets hypehia cak ordicio the ocplan acrop demmie\n",
      "eentileel enicungible riebetar the wefele alls dioirol tan harbunaineram one tf af\n",
      "fvill warroo ate astsitatioura obsilonn sisiented rossola emistorrioss anse weiea \n",
      " z on compliels eiship an gernoaned tadeer sound thenbsuftious one sioth on for sh\n",
      "hvasteicahly scaleeseitieen tia illeags tayeers the nas inclusor siotsens onlea au\n",
      "================================================================================\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 6100: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 5.06\n",
      "Average loss at step 6200: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.90\n",
      "Average loss at step 6300: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.38\n",
      "Average loss at step 6400: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.79\n",
      "Average loss at step 6500: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.49\n",
      "Average loss at step 6600: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 5.22\n",
      "Average loss at step 6700: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.95\n",
      "Average loss at step 6800: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.43\n",
      "Average loss at step 6900: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 5.54\n",
      "Average loss at step 7000: 1.000000 learning rate: 1.000000\n",
      "Minibatch perplexity : 4.69\n",
      "================================================================================\n",
      "uh illos matant by alnowicaliaioraierr is all ida rile inscent s virants ancogns t\n",
      "tt cuwseleuf asesheer great less birlneque ranuisisil naturol and niu proses in ot\n",
      "tferia jem well onrun to boarso y worldo mosses herinauookitionitsed role or de re\n",
      "ego sirondr one nine sis of duspined oreirhn from killelosaodws neetfepic worner o\n",
      "oureno geoneawien cants wese dot duin canbretio in wnolish irmnussiosal vlv this w\n",
      "================================================================================\n",
      "Validation set perplexity: 6.20\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency= 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Global vars initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = {}\n",
    "        for i in range(num_unrollings+1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _,l,predictions,lr = session.run([optimizer,loss,train_prediction,learning_rate],feed_dict=feed_dict)\n",
    "        mean_loss += 1\n",
    "        if step%summary_frequency==0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss=0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity : %.2f' %float( np.exp(logprob(predictions,labels))))\n",
    "        if step % (summary_frequency * 10) ==0:\n",
    "            # generate samples\n",
    "            print('=' * 80)\n",
    "            for _ in range(5):\n",
    "                sentence = characters(feed)[0]\n",
    "                feed = sample(random_distribution())\n",
    "                sentence += characters(feed)[0]\n",
    "                reset_sample_state.run()\n",
    "                for _ in range(80):\n",
    "                    prediction = sample_prediction.eval({sample_input:feed})\n",
    "                    feed = sample(prediction)\n",
    "                    sentence += characters(feed)[0]\n",
    "                print(sentence)    \n",
    "            print('=' * 80)\n",
    "            #measure validation set perplexity\n",
    "            \n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                \n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
